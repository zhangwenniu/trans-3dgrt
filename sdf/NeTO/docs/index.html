<!DOCTYPE html
        PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <!--    <title> <br>  </title>-->
    <title>NeTO: Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <!-- Meta tags for Zotero grab citation -->
    <meta name="citation_title"
          content="NeTO: Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing">
    <meta name="citation_author" content="Zongcheng, Li">
    <meta name="citation_author" content="Long, Xiaoxiao">

    <meta name="citation_author" content="Yusen, Wang">
    <meta name="citation_author" content="Tuo, Cao">
    <meta name="citation_author" content="Wang, Wenping">
    <meta name="citation_author" content="Fei, Luo">
    <meta name="citation_author" content="Chunxia, Xiao">
    <meta name="citation_publication_date" content="2023">
    <meta name="citation_conference_title" content="ARXIV">
    <meta name="citation_pdf_url" content="">

    <meta name="robots" content="index,follow">
    <meta name="description"
          content="
          We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. 
          Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena.
          Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing.
          In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. 
          The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the self-occlusion aware strategy makes it possible for our method to accurately reconstruct the self-occluded regions. 
          Experiments show that our method achieves faithful reconstruction results and outperforms prior works by a large margin.
		">
    <link rel="author" href="https://www.xxlong.site/"/>


    <!-- Fonts and stuff -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800'
          rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen"/>
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css"/>
    <script src="js/google-code-prettify/prettify.js"></script>
</head>


<body>

<div id="content">
    <div id="content-inner">
        <div class="section logos" style="text-align:center">
            <a href="https://www.whu.edu.cn/" target="_blank"><IMG src="./logos/logo_WHU.png" height="35"
                border="0"></a></td>
            <a href="https://www.hku.hk/" target="_blank"><IMG src="./logos/Logo_HKU.png" height="35"
                                                               border="0"></a></td>
           
            <a href="https://www.tamu.edu/" target="_blank"><IMG src="./logos/logo_TAMU.png" height="35"
                                                                 border="0"></a></td>
        </div>

        <div class="section head">

            <h1>NeTO: Neural Reconstruction of Transparent Objects <br>
                with Self-Occlusion Aware Refraction-Tracing </h1>
                 
            <div class="authors">
		        <a  >Zongcheng Li*</a><sup>1</sup>&#160;&#160;
                <a href="https://www.xxlong.site/" target="_blank">Xiaoxiao Long*</a><sup> 2</sup>&#160;&#160;
                <a  >Yusen Wang</a><sup>1</sup>&#160;&#160;
                <a  >Tuo Cao</a><sup>1</sup>&#160;&#160;
                <a href="https://www.cs.hku.hk/people/academic-staff/wenping/">Wenping Wang</a><sup> 3</sup>&#160;&#160;
                <a  >Fei Luo</a><sup>1</sup>&#160;&#160;
		        <a href="http://graphvision.whu.edu.cn/">Chunxia Xiao</a><sup> 1</sup>&#160;&#160;
            </div>

            <div class="affiliations">
                <sup>1</sup><a href="https://www.whu.edu.cn/" target="_blank">Wuhan University</a>&#160;&#160;
		        <sup>2</sup><a href="https://www.hku.hk/" target="_blank">The University of Hong Kong</a>&#160;&#160;
                <sup>3</sup><a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>&#160;&#160;
            </div>
            

            <div class="Eq"><sup>*</sup><a>Equal contributions</a></div>
            <div class="venue"><a href="" target="_blank">ICCV 2023 </a></div>
                
            <div class="section downloads">
                <!--<h2>Downloads</h2>-->
                <center>
                    <ul>
                        <li class="grid">
                            <div class="griditem">
                                <a href="https://arxiv.org/pdf/2303.11219.pdf" target="_blank"
                                   class="imageLink"><img
                                        src="./images/pdf.png"></a><br/>
                                <a href="./ARXIV_NeTO.pdf">Paper</a>
                            </div>
                        </li>
                        <li class="grid">
                            <div class="griditem">
                                <a href="https://github.com/xxlong0/NeTO" target="_blank"
                                   class="imageLink"><img
                                        src="./images/data_ico.png"></a><br/>
                                <a href="https://github.com/xxlong0/NeTO">Code</a>

                            </div>
                        </li>

                    </ul>
                </center>
            </div>
        </div>

        <div class="section abstract">
            <h2>Abstract</h2><br>
            <div class="row" style="margin-bottom:5px">
                <div class="col" style="text-align:center">
                    <img src="./images/teaser.png" style="width:1100px;height:auto;"
                         style="width:70%; margin-bottom:20px">

                </div>

            </div>

            <p>
                We present a novel method called NeTO, for capturing the 3D geometry of solid transparent objects from 2D images via volume rendering. 
                Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena.
                Although existing refraction-tracing-based methods, designed especially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing.
                In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. 
                The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of views, and the self-occlusion aware strategy makes it possible for our method to accurately reconstruct the self-occluded regions. 
                Experiments show that our method achieves faithful reconstruction results and outperforms prior works by a large margin.
            </p>
            <!--            </p>-->
        </div>


 
        <div class="section abstract">
            <h2>Comparisons on DRT dataset with sparse views</h2>
            <p>
                To validate the ability of NeTO to reconstuct transparent surfaces, 
                we conduct comparisons with current state-of-the-art method on the DRT dataset and our real data.
dataset. As you can see, with sparse views, our results outperform DRT
                in terms of model completeness and accuracy. 
                <!-- In contrast, due to the flexibility of sdf representation, our method successfully model the open surfaces. -->
            </p>
            <center>
                <!-- <iframe width="640" height="360" src="./mp4/composite3.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
                <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/dog.mp4" type="video/mp4">
                </video> -->
                <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/horse.mp4" type="video/mp4">
                </video>
<!--                <video width="60%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">-->
<!--                    <source src="./videos/garment2.mp4" type="video/mp4">-->
<!--                </video>-->
            </center>
        </div>

        <div class="section abstract">
            <h2>Comparisons on DRT dataset with Full views</h2>
            <p>
                When we make use of more views, e.g., full views, the reconstruction results of ours and DRT 
                are improved compared with reconstruc- tions with sparse views. 
    
            </p>
            <center>
    
                <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/pig.mp4" type="video/mp4">
                </video>
                <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/monkey.mp4" type="video/mp4">
                </video>
            </center>
        </div>

        <div class="section abstract">
            <h2>Comparisons with DRT on our real data </h2>
            We further conduct evaluation on a self-collected real Bull and Mouse object. Our method
            accurately recovers the geometry with clean and smooth
            surfaces, while DRT mistakenly reconstructs surfaces with
            noises.
            <center>
           
                <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/bull.mp4" type="video/mp4">
                </video>
                <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="./videos/mouse.mp4" type="video/mp4">
                </video>
            </center>
        </div>


                <div class="section abstract">
                    <h2>Citation</h2>
                    <div class="section bibtex" style="text-align:left; margin-left: 40px; margin-right: 40px">
        					<pre>
@article{li2023neto,
  title={NeTO: Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing},
  author={Li, Zongcheng and Long, Xiaoxiao and Wang, Yusen and Cao, Tuo and Wang, Wenping and Luo, Fei and Xiao, Chunxia},
  journal={arXiv preprint arXiv:2303.11219},
  year={2023}
}
                    </div>
                </div>



        <div class="section">
            <hr class="smooth">
            This page is <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. Page last
            updated
            <script type="text/javascript">
                var m = "This page was last updated: " + document.lastModified;
                var p = m.length - 9;
                document.writeln("<left>");
                document.write(m.substring(p, 0) + ".");
                document.writeln("</left>");
            </script>
        </div>
    </div>
</div>
</body>
</html>
